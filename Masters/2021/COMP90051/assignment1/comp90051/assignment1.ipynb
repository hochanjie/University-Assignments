{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import itertools\n",
    "import networkx as nx\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train data\n",
    "def loadTrainData():\n",
    "    filename = \"train.txt\"\n",
    "    return [line.rstrip(\"\\n\") for line in open(filename)]\n",
    "\n",
    "# this is your graph function, mainly use this data structure to generate psuedo false example\n",
    "def createUndirectedGraph(dataRow):\n",
    "    g = nx.Graph()\n",
    "    for row in dataRow:\n",
    "        authorIds = row.split()\n",
    "        for i, author in enumerate(authorIds):\n",
    "            for coauthor in authorIds[i+1:]:\n",
    "                if g.has_edge(author, coauthor):\n",
    "                    g[author][coauthor]['frequency'] += 1 # TODO: train this (e.g. noise in the training data)\n",
    "                else:\n",
    "                    g.add_edge(author, coauthor, frequency=1)\n",
    "    return g\n",
    "\n",
    "# preprocessing train data\n",
    "def pre_row(trainRow):\n",
    "    txt = [list(map(int, i.split())) for i in trainRow]\n",
    "    trainGraph = createUndirectedGraph(trainRow)\n",
    "    g1 = {}\n",
    "    converted_txt = []\n",
    "    tmp = []\n",
    "    for link in txt:\n",
    "        for subset in itertools.permutations(link, 2):\n",
    "            tmp.append(subset[0])\n",
    "            tmp.append(subset[1])\n",
    "            converted_txt.append(subset)\n",
    "    train1 = pd.DataFrame(converted_txt, columns=[\"srce\", \"dest\"])\n",
    "    freq = train1.groupby([\"srce\", \"dest\"]).size().values\n",
    "    #train1['freq'] = freq\n",
    "    txt_1 = sorted(set(converted_txt))\n",
    "    df1 = pd.DataFrame(txt_1, columns=[\"srce\", \"dest\"])\n",
    "    #df1['freq'] = freq\n",
    "    df1['score'] = 1\n",
    "    for points, f in zip(txt_1, freq):\n",
    "        g1[points[0]] = g1.setdefault(points[0], [])\n",
    "        g1[points[0]].append((points[1], f))\n",
    "    V = list(set(tmp))\n",
    "    return g1, V, df1, txt_1\n",
    "\n",
    "#functions that generating features \n",
    "def fu(g1, u):\n",
    "    return 1\n",
    "\n",
    "def getNodeScore(g1, Nu):\n",
    "    AA = 0\n",
    "    RA = 0\n",
    "    CCN = len(Nu)\n",
    "    CRA = 0\n",
    "    if len(Nu) == 0:\n",
    "        return 0, 0, 0, 0\n",
    "    for u in Nu:\n",
    "        AA += 1/np.log(len(g1[u]))\n",
    "        RA += 1/len(g1[u])\n",
    "        CCN += fu(g1, u)\n",
    "        CRA += fu(g1, u)/len(g1[u])\n",
    "    return AA, RA, CCN, CRA\n",
    "\n",
    "def Pxy(g1, x, y):\n",
    "    try:\n",
    "        Gx = g1[x]\n",
    "        Gy = g1[y]\n",
    "    except:\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    Nx = [i[0] for i in Gx]\n",
    "    Ny = [i[0] for i in Gy]\n",
    "    NxINy = list(set(Nx) & set(Ny))\n",
    "    NxUNy = list(set(Nx + Ny))\n",
    "    AA, RA, CCN, CRA = getNodeScore(g1, NxINy)\n",
    "    cardNx = len(Nx)\n",
    "    cardNy = len(Ny)\n",
    "    PA = cardNx * cardNy\n",
    "    JC = len(NxINy)/len(NxUNy)\n",
    "    HPI = len(NxINy)/min(cardNx, cardNy)\n",
    "    HDI = len(NxINy)/max(cardNx, cardNy)\n",
    "    return AA, len(NxINy), len(NxINy)/np.sqrt(cardNx*cardNy), JC, HPI, HDI, PA, RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data \n",
    "test = pd.read_csv(\"test-public.csv\")\n",
    "train1 = loadTrainData()\n",
    "\n",
    "# an attempt to split the data into train and dev set\n",
    "random.shuffle(train1)\n",
    "m = int(len(train1)*0.9)\n",
    "trainRow = train1[0:m]\n",
    "devRow = train1[m:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only use train set to construct the model\n",
    "g1, V, df1, txt_1 = pre_row(trainRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this structure use the whole dataset in order to evaluate the model\n",
    "trainGraph  = createUndirectedGraph(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate psudo-false-example\n",
    "psuedo = []\n",
    "i = 0\n",
    "while len(psuedo) < len(txt_1):\n",
    "    random.seed(i)\n",
    "    nodes = random.sample(V, 2)\n",
    "    n1, n2 = nodes[0], nodes[1]\n",
    "    try:\n",
    "        path = nx.dijkstra_path(trainGraph, source = n1, target = n2)\n",
    "    except:\n",
    "        if [n1, n2, 0] not in psuedo:\n",
    "            psuedo.append([n1, n2, 0])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add psudo example into train data set\n",
    "df2 = pd.DataFrame(psuedo, columns = ['srce', 'dest', 'score'])\n",
    "df3 = df1.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate features\n",
    "AAs, CNs, SIs, JCs, HPIs, HDIs, PAs, RAs = [],[],[],[],[],[],[],[]\n",
    "for idx, i in df3.iterrows():\n",
    "    n1 = i[0]\n",
    "    n2 = i[1]\n",
    "    AA, CN, SI, JC, HPI, HDI, PA, RA = Pxy(g1, n1, n2)\n",
    "    AAs.append(AA)\n",
    "    CNs.append(CN)\n",
    "    SIs.append(SI)\n",
    "    JCs.append(JC)\n",
    "    HPIs.append(HPI)\n",
    "    HDIs.append(HDI)\n",
    "    PAs.append(PA)\n",
    "    RAs.append(RA)\n",
    "    \n",
    "df3['AA'] = AAs\n",
    "df3['CN'] = CNs\n",
    "df3['SI'] = SIs\n",
    "df3['JC'] = JCs\n",
    "df3['HPI'] = HPIs\n",
    "df3['HDI'] = HDIs\n",
    "df3['PA'] = PAs\n",
    "df3['RA'] = RAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "X = df3[['AA', 'CN', 'SI', 'JC', 'HPI', 'HDI', 'PA', 'RA']]\n",
    "y = df3['score']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 GaussianNB, not good, currently discard\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train1 = sc.fit_transform(X_train)\n",
    "X_test1 = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9424924332148967\n",
      "[[7560  117]\n",
      " [ 757 6764]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train1, y_train)\n",
    "y_pred = classifier.predict(X_test1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test,y_pred)\n",
    "print(acc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.176453\n",
      "         Iterations 12\n"
     ]
    }
   ],
   "source": [
    "# model2, logistic regression, not good, currently discard\n",
    "features = ['AA', 'CN', 'SI', 'JC', 'HPI', 'HDI', 'PA', 'RA']\n",
    "X_train2 = X_train[features]\n",
    "X_test2 = X_test[features]\n",
    "X_test2['intercept'] = 1.0\n",
    "X_train2['intercept'] = 1.0\n",
    "logit = sm.Logit(y_train, X_train2)\n",
    "result = logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  score   No. Observations:                45594\n",
      "Model:                          Logit   Df Residuals:                    45585\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Fri, 09 Apr 2021   Pseudo R-squ.:                  0.7454\n",
      "Time:                        22:13:46   Log-Likelihood:                -8045.2\n",
      "converged:                       True   LL-Null:                       -31603.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "AA            25.4031      2.956      8.593      0.000      19.609      31.197\n",
      "CN            -5.3453      0.596     -8.964      0.000      -6.514      -4.177\n",
      "SI            45.0351      3.323     13.552      0.000      38.522      51.548\n",
      "JC           -69.0229      3.969    -17.389      0.000     -76.803     -61.243\n",
      "HPI           -1.2528      0.738     -1.697      0.090      -2.699       0.194\n",
      "HDI           18.4818      4.326      4.273      0.000      10.004      26.960\n",
      "PA             0.0011   7.69e-05     13.861      0.000       0.001       0.001\n",
      "RA           -39.4511      6.595     -5.982      0.000     -52.378     -26.524\n",
      "intercept     -2.5631      0.025   -101.453      0.000      -2.613      -2.514\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.26 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting probability and use threshold of 0.5 to determine labels\n",
    "tmp = X_test2.dot(result.params)\n",
    "y_pred = 1/(1+np.exp(-tmp))\n",
    "y_pred1 = []\n",
    "for i in y_pred:\n",
    "    if i > 0.5:\n",
    "        y_pred1.append(1)\n",
    "    else:\n",
    "        y_pred1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9496644295302014\n",
      "[[7544  133]\n",
      " [ 632 6889]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred1)\n",
    "ac = accuracy_score(y_test,y_pred1)\n",
    "print(ac)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data structure for AUC and precision evaulation for logistic regression\n",
    "X_psuedo = []\n",
    "for i in psuedo:\n",
    "    n1 = i[0]\n",
    "    n2 = i[1]\n",
    "    AA, CN, SI, JC, HPI, HDI, PA, RA = Pxy(g1, n1, n2)\n",
    "    X_psuedo.append([AA, CN, SI, JC, HPI, HDI, PA, RA, 1])\n",
    "new = np.array(X_psuedo)\n",
    "tmp1 = new.dot(result.params)\n",
    "y_psuedo = 1/(1+np.exp(-tmp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = []\n",
    "g_dev, V_dev, df_dev, txt_dev = pre_row(devRow) \n",
    "for i in txt_dev:\n",
    "    n1 = i[0]\n",
    "    n2 = i[1]\n",
    "    AA, CN, SI, JC, HPI, HDI, PA, RA = Pxy(g1, n1, n2)\n",
    "    X_dev.append([AA, CN, SI, JC, HPI, HDI, PA, RA, 1])\n",
    "new = np.array(X_dev)\n",
    "tmp1 = new.dot(result.params)\n",
    "y_dev = 1/(1+np.exp(-tmp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalution function implemented, mentioned in paper https://arxiv.org/pdf/1010.0725.pdf\n",
    "def AUC(R, M):\n",
    "    nume = 0\n",
    "    deno = 0\n",
    "    idx = 0\n",
    "    for i in R:\n",
    "        deno += len(M)\n",
    "        nume += sum(i > j for j in M)\n",
    "        nume += 0.5*list(M).count(i)\n",
    "        if idx%100 == 0:\n",
    "            pass\n",
    "            #print(nume, deno, nume/deno)\n",
    "        idx += 1\n",
    "    return nume/deno\n",
    "\n",
    "def precision(R, M, thres):\n",
    "    s1 = sum(i > thres for i in R)\n",
    "    s2 = sum(j > thres for j in M)\n",
    "    return (s1)/(s1+s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824787\n",
      "0.8455821635012386\n"
     ]
    }
   ],
   "source": [
    "# randomly pick 1000 instances to test, otherwise it takes too long to calculate\n",
    "n_dev = random.sample(list(y_dev), 1000)\n",
    "n_psuedo = random.sample(list(y_psuedo), 1000)\n",
    "print(AUC(n_dev, n_psuedo))\n",
    "print(precision(y_dev, y_psuedo, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try to draw something\n",
    "import matplotlib.pyplot as plt\n",
    "thres = np.array(list(range(0,11)))/10\n",
    "prec_list = []\n",
    "for i in thres:\n",
    "    prec_list.append(precision(y_dev, y_psuedo, i))\n",
    "plt.plot(thres, prec_list)\n",
    "plt.xlabel('threshold')\n",
    "plt.ylabel('precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2000,7) and (9,) not aligned: 7 (dim 1) != 9 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fd14b4841be1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHPI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtmp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtmp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Predicted'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2000,7) and (9,) not aligned: 7 (dim 1) != 9 (dim 0)"
     ]
    }
   ],
   "source": [
    "# trying to predict test and output to csv file, currently discard\n",
    "pre = []\n",
    "for idx, i in test.iterrows():\n",
    "    n1 = i[1]\n",
    "    n2 = i[2]\n",
    "    AA, CN, SI, JC, HPI, HDI, PA, RA  = Pxy(g1, n1, n2)\n",
    "    pre.append([AA, CN, SI, JC, HPI, PA, 1])\n",
    "new = np.array(pre)\n",
    "tmp1 = new.dot(result.params)\n",
    "y_pred = 1/(1+np.exp(-tmp1))\n",
    "test['Predicted'] = y_pred\n",
    "test[['Id', 'Predicted']].to_csv('results/EdgeAddClassifier.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold attempt, not really implemented here, whole loop is in ass1.py\n",
    "kf = KFold(n_splits=5,shuffle=False)\n",
    "traindf = np.array(trainRow)\n",
    "for train_index, dev_index in kf.split(trainRow):\n",
    "    X_train5, X_dev5 = traindf[train_index], traindf[dev_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dev1, V_dev1, df_dev1, txt_dev1 = pre_row(X_train5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovo', degree=3, gamma=20, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svm model\n",
    "from sklearn import svm\n",
    "clf1 = svm.SVC(C = 1, kernel = 'rbf', gamma = 20, decision_function_shape = 'ovo')\n",
    "clf1.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9586568408123876\n"
     ]
    }
   ],
   "source": [
    "print(clf1.score(X_train2, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9480852743782077\n"
     ]
    }
   ],
   "source": [
    "print(clf1.score(X_test2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting dev set\n",
    "X_dev = []\n",
    "g_dev, V_dev, df_dev, txt_dev = pre_row(devRow) \n",
    "for i in txt_dev:\n",
    "    n1 = i[0]\n",
    "    n2 = i[1]\n",
    "    AA, CN, SI, JC, HPI, HDI, PA, RA  = Pxy(g1, n1, n2)\n",
    "    X_dev.append([AA, CN, SI, JC, HPI, HDI, PA, RA, 1])\n",
    "y_dev = clf1.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting test set\n",
    "pre = []\n",
    "for idx, i in test.iterrows():\n",
    "    n1 = i[1]\n",
    "    n2 = i[2]\n",
    "    AA, CN, SI, JC, HPI, HDI, PA, RA  = Pxy(g1, n1, n2)\n",
    "    pre.append([AA, CN, SI, JC, HPI, HDI, PA, RA, 1])\n",
    "y_pred = clf1.predict(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting psuedo set\n",
    "X_psuedo = []\n",
    "for i in psuedo:\n",
    "    n1 = i[0]\n",
    "    n2 = i[1]\n",
    "    AA, CN, SI, JC, HPI, HDI, PA, RA = Pxy(g1, n1, n2)\n",
    "    X_psuedo.append([AA, CN, SI, JC, HPI, HDI, PA, RA, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating probability for each instance in dev set and test set\n",
    "decision_f = clf1.decision_function(pre)\n",
    "decision_f1 = clf1.decision_function(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate probability for each instance in psuedo set\n",
    "decision_f2 = clf1.decision_function(X_psuedo)\n",
    "# normalize probability\n",
    "map_psuedo = []\n",
    "max_d = max(decision_f2)\n",
    "min_d = min(decision_f2)\n",
    "for i in decision_f2:\n",
    "    map_psuedo.append((i-min_d)/(max_d - min_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize prob.\n",
    "map_dev = []\n",
    "max_d = max(decision_f1)\n",
    "min_d = min(decision_f1)\n",
    "for i in decision_f1:\n",
    "    map_dev.append((i-min_d)/(max_d - min_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize prob.\n",
    "map_pred = []\n",
    "max_d = max(decision_f)\n",
    "min_d = min(decision_f)\n",
    "for i in decision_f:\n",
    "    map_pred.append((i-min_d)/(max_d - min_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813581\n",
      "0.8536863729742068\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "n_dev = random.sample(list(map_dev), 1000)\n",
    "n_psuedo = random.sample(list(map_psuedo), 1000)\n",
    "print(AUC(n_dev, n_psuedo))\n",
    "print(precision(map_dev, map_psuedo, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this output get kaggle score above 0.8\n",
    "test['Predicted'] = map_pred\n",
    "test[['Id', 'Predicted']].to_csv('results/EdgeAddClassifier.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
