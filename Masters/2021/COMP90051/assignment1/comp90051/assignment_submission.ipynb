{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import sklearn.utils\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train data\n",
    "def loadTrainData():\n",
    "    filename = \"train.txt\"\n",
    "    return [line.rstrip(\"\\n\") for line in open(filename)]\n",
    "\n",
    "def loadTrainDataAsUndirectedGraph():\n",
    "    filename = \"train.txt\"\n",
    "    rows = [line.rstrip(\"\\n\") for line in open(filename)]\n",
    "    g = nx.Graph()\n",
    "    for row in rows:\n",
    "        authorIds = row.split()\n",
    "        for i, author in enumerate(authorIds):\n",
    "            for coauthor in authorIds[i+1:]:\n",
    "                if g.has_edge(author, coauthor):\n",
    "                    g[author][coauthor]['frequency'] += 1\n",
    "                else:\n",
    "                    g.add_edge(author, coauthor, frequency=1)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Balanced Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function attempt to create a balanced training data with false instances\n",
    "def createBalancedTrainingData(graph, testDF):\n",
    "    trueInstances = [[n1, n2, 1] for (n1, n2) in graph.edges if graph[n1][n2]['frequency'] > 1] \n",
    "    sourceSink = testDF[\"Source-Sink\"].tolist()\n",
    "    adj_G = nx.to_numpy_matrix(graph, nodelist = graph.nodes)\n",
    "    # get unconnected node-pairs\n",
    "    f1 = []\n",
    "    f2 = []\n",
    "\n",
    "    # traverse adjacency matrix\n",
    "    l = int(0.5 * len(trueInstances))\n",
    "    while len(f1) < l or len(f2) < l:\n",
    "        #random.seed() # Removed seed\n",
    "        i = random.sample(range(adj_G.shape[0]), 1)[0]\n",
    "        j = random.sample(range(adj_G.shape[1]), 1)[0]\n",
    "        try:\n",
    "            # If shortest path > 4 then it goes into half of the false instances\n",
    "            if 4 < nx.shortest_path_length(graph, str(i), str(j)):\n",
    "                if len(f2) < l and (i,j) not in sourceSink:\n",
    "                    #print(i, j)\n",
    "                    f2.append([str(i), str(j), 0])\n",
    "        except:\n",
    "            # if there is no path between two nodes, then it goes into another half of the false instances.\n",
    "            if len(f1) < l and (i,j) not in sourceSink:\n",
    "                f1.append([str(i), str(j), 0])\n",
    "        \n",
    "    print('len(f1):',len(f1))\n",
    "    print('len(f2):',len(f2))\n",
    "    print('len(trueInstances):',len(trueInstances))\n",
    "    # combine and shuffle them into dataframe\n",
    "    data = sklearn.utils.shuffle(f1+f2+trueInstances)\n",
    "    return pd.DataFrame(data,columns=['Source','Sink','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chunk is for preprocessing the network graph and gathering data to compute features\n",
    "import community as community_louvain\n",
    "trainGraph = loadTrainDataAsUndirectedGraph() # create network graph\n",
    "\n",
    "# this returns a list of dictionaries containing subgraphs and nodes within each subgraph\n",
    "components = list(nx.connected_components(trainGraph)) \n",
    "# this returns a list of nodes in network graph\n",
    "nodes = list(trainGraph.nodes) \n",
    "component_dict = {}\n",
    "for j in nodes:\n",
    "    for idx, i in enumerate(components):\n",
    "        tmp = list(i)\n",
    "        #print(tmp)\n",
    "        if str(j) in tmp:\n",
    "            component_dict[j] = np.log(len(i))\n",
    "\n",
    "# partition is used to calculate community-related features\n",
    "partition = community_louvain.best_partition(trainGraph)\n",
    "# betweenness and eigenvector centrality are two quantities which measure the importance of the nodes.\n",
    "betweenness_dict = nx.betweenness_centrality(trainGraph)\n",
    "eigenvector_dict = nx.eigenvector_centrality(trainGraph)\n",
    "\n",
    "nx.set_node_attributes(trainGraph, component_dict, 'component')\n",
    "nx.set_node_attributes(trainGraph, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(trainGraph, eigenvector_dict, 'eigenvector')\n",
    "nx.set_node_attributes(trainGraph, partition, 'community')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into dataframe\n",
    "testDF = pd.read_csv('test-public.csv', converters = {'Source': str, 'Sink': str})\n",
    "testDF['Source-Sink'] = list(zip(testDF['Source'], testDF['Sink']))\n",
    "# create training dataframe\n",
    "trainDF = createBalancedTrainingData(trainGraph, testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions generating features, details descriptions are in the reports.\n",
    "# not all functions are used in the training, some of them are discarded.\n",
    "def shortestDistance(graph, n1, n2):\n",
    "    try: return 1/nx.shortest_path_length(graph, n1, n2)\n",
    "    except: return 0\n",
    "    \n",
    "def commonNeighbours(graph, n1, n2):\n",
    "    try: return len(list(nx.common_neighbors(graph, n1, n2)))\n",
    "    except: return 0\n",
    "\n",
    "def jaccard(graph, n1, n2):\n",
    "    try: return list(nx.jaccard_coefficient(graph, [(n1, n2)]))[0][2]\n",
    "    except: return 0\n",
    "    \n",
    "def adamicAdar(graph, n1, n2):\n",
    "    try: return list(nx.adamic_adar_index(graph, [(n1, n2)]))[0][2]\n",
    "    except: return 0\n",
    "\n",
    "def preferentialAttachment(graph, n1, n2):\n",
    "    try: return list(nx.preferential_attachment(graph, [(n1, n2)]))[0][2]\n",
    "    except: return 0\n",
    "\n",
    "def resourceAllocation(graph, n1, n2):\n",
    "    try: return list(nx.resource_allocation_index(graph, [(n1, n2)]))[0][2]\n",
    "    except: return 0\n",
    "    \n",
    "def localPath(graph, n1, n2):\n",
    "    try:\n",
    "        paths = list(nx.all_simple_paths(graph, source=n1, target=n2, cutoff=3))\n",
    "        A2 = 0.0\n",
    "        A3 = 0.0\n",
    "        A1 = 0.0\n",
    "        for path in paths:\n",
    "            if len(path) == 3:\n",
    "                A2 = A2 + 1.0\n",
    "            elif len(path) == 4:\n",
    "                A3 = A3 + 1.0\n",
    "            elif len(path) == 2:\n",
    "                A1 = A1 + 1.0\n",
    "        return A1 + 0.1*A2 + 0.01*A3\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def Bet(graph, n1, n2):\n",
    "    try:\n",
    "        b1 = 1/graph.nodes[n1]['betweenness']\n",
    "        b2 = 1/graph.nodes[n2]['betweenness']\n",
    "    except:\n",
    "        b1 = 0\n",
    "        b2 = 0\n",
    "    return max(b1, b2)\n",
    "\n",
    "def Eig(graph, n1, n2):\n",
    "    try:\n",
    "        e1 = 1/graph.nodes[n1]['eigenvector']\n",
    "        e2 = 1/graph.nodes[n2]['eigenvector']\n",
    "    except:\n",
    "        e1 = 0\n",
    "        e2 = 0\n",
    "    return max(e1, e2)\n",
    "\n",
    "def com_ra(graph, n1, n2):\n",
    "    try:\n",
    "        return list(nx.ra_index_soundarajan_hopcroft(trainGraph, [(n1, n2)]))[0][2]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def component(graph, n1, n2):\n",
    "    try:\n",
    "        e1 = graph.nodes[n1]['component']\n",
    "        e2 = graph.nodes[n2]['component']\n",
    "    except:\n",
    "        e1 = 0\n",
    "        e2 = 0\n",
    "    return max(e1, e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFeaturesToDataframe(graph, data):\n",
    "    #data['CommonNeighbours'] = data.apply(lambda l: commonNeighbours(graph, l.Source, l.Sink), axis=1)\n",
    "    #print('Added \"CommonNeighbours\" column')\n",
    "    #data['Jaccard'] = data.apply(lambda l: jaccard(graph, l.Source, l.Sink), axis=1)\n",
    "    #print('Added \"Jaccard\" column')\n",
    "    data['AdamicAdar'] = data.apply(lambda l: adamicAdar(graph, l.Source, l.Sink), axis=1)\n",
    "    print('Added \"AdamicAdar\" column')\n",
    "#     data['PreferentialAttachment'] = data.apply(lambda l: preferentialAttachment(graph, l.Source, l.Sink), axis=1)\n",
    "#     print('Added \"PreferentialAttachment\" column')\n",
    "#     data['ResourceAllocation'] = data.apply(lambda l: resourceAllocation(graph, l.Source, l.Sink), axis=1)\n",
    "#     print('Added \"ResourceAllocation\" column')\n",
    "#     data['Dist'] = data.apply(lambda l: shortestDistance(graph, l.Source, l.Sink),\n",
    "#                               axis=1)  ## can't just leave as highest number cause will be detrimental when normalising\n",
    "#     print('Added \"Dist\" column')\n",
    "\n",
    "    # newly-added features\n",
    "    # community common nodes\n",
    "    data['CCN'] = data.apply(lambda l: com_ra(graph, l.Source, l.Sink), axis=1)\n",
    "    print('Added \"CCN\" column')\n",
    "    # betweenness centrality\n",
    "    data['Betweeness'] = data.apply(lambda l: Bet(graph, l.Source, l.Sink), axis=1)\n",
    "    print('Added \"Betweeness\" column')\n",
    "    # eigenvector centrality\n",
    "    data['Eigenvector'] = data.apply(lambda l: Eig(graph, l.Source, l.Sink), axis=1)\n",
    "    print('Added \"Eigenvector\" column')\n",
    "    # the size of the component nodes are in.\n",
    "    data['Component'] = data.apply(lambda l: component(graph, l.Source, l.Sink), axis=1)\n",
    "    print('Added \"Component\" column')\n",
    "    # the number of paths of different lengths are considered as an accumulative weight\n",
    "    data['LP'] = data.apply(lambda l: localPath(graph, l.Source, l.Sink), axis=1)\n",
    "    print('Added \"LP\" column')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addFeaturesToDataframe(trainGraph, trainDF)\n",
    "addFeaturesToDataframe(trainGraph, testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureColumns = [value for value in trainDF.columns if value not in ['Source', 'Sink', 'Label', 'RA', 'PA', 'JC', 'CN', 'Frequency', 'Component', 'CCN']] # 'SI', 'HPI', 'LP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of feature distribution (comparing success vs failure scenarios)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.distributions.distplot(trainDF['Label']).set_title(f'Distribution of Label')\n",
    "plt.show()\n",
    "\n",
    "for name in ['CommonNeighbours', 'Jaccard', 'AdamicAdar','PreferentialAttachment', 'ResourceAllocation', 'Dist', 'SI', 'HPI', 'LP']:\n",
    "    sns.distributions.distplot(trainDF[trainDF['Label']==0][name]).set_title(f'Distribution of {name} when label=0')\n",
    "    plt.show()\n",
    "    sns.distributions.distplot(trainDF[trainDF['Label']==1][name]).set_title(f'Distribution of {name} when label=1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "partition = community_louvain.best_partition(trainGraph)\n",
    "# draw network graph showing partitions as different subgraphs\n",
    "pos = nx.spring_layout(trainGraph)\n",
    "# color the nodes according to their partitions\n",
    "cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(trainGraph, pos, partition.keys(), node_size=40,\n",
    "                       cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(trainGraph, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "X_train = trainDF[FeatureColumns]\n",
    "X_test = testDF[FeatureColumns]\n",
    "y_train = trainDF['Label']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train.loc[:, FeatureColumns] = scaler.fit_transform(X_train)\n",
    "X_test.loc[:, FeatureColumns] = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Wrapper method\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# lr = LogisticRegression(class_weight=\"balanced\")\n",
    "# selector = RFE(lr, n_features_to_select=3, step=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# allF = pd.DataFrame({'features': X_train.columns,'importance': selector.ranking_})\n",
    "# importantFeatures = list(allF.query('importance==1')['features'])\n",
    "# importantFeatures.sort()\n",
    "# print('importantFeatures = ', importantFeatures)\n",
    "# allF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter method (LP, AdamicAdar)\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "s = SelectKBest(f_classif, k=4)\n",
    "s.fit(X_train, y_train)\n",
    "allF = pd.DataFrame({'features': X_train.columns,'scores': s.scores_, 'pvalue':s.pvalues_}).sort_values(by=['scores'],ascending=False)\n",
    "importantFeatures = list(allF['features'])[:7]\n",
    "importantFeatures.sort()\n",
    "print('importantFeatures = ', importantFeatures)\n",
    "allF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[importantFeatures]\n",
    "X_test = X_test[importantFeatures]\n",
    "testIds = testDF['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "neuralNetworkUndirected = MLPClassifier(hidden_layer_sizes=(3), momentum=True, max_iter=300)\n",
    "NN_output = cross_val_score(neuralNetworkUndirected, X_train, y_train, scoring='roc_auc', cv=5)\n",
    "print('Undirected Scores: ',NN_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNetworkUndirected.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_pred = neuralNetworkUndirected.predict_proba(X_test)\n",
    "neuralNetworkResultUndirected =  pd.DataFrame({'Id': testIds,'Predicted': NN_pred[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNetworkResultUndirected.to_csv('results/neuralNetwork.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# parameters = {\n",
    "#     'solver': ['lbfgs'],\n",
    "#     'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ],\n",
    "#     'alpha': 10.0 ** -np.arange(1, 10),\n",
    "#     'hidden_layer_sizes':np.arange(1, 10)\n",
    "# }\n",
    "# cv = GridSearchCV(MLPClassifier(), parameters, scoring='roc_auc')\n",
    "# cv.fit(X_train, y_train)\n",
    "\n",
    "# print(\"tuned hpyerparameters :(best parameters) \",cv.best_params_)\n",
    "# print(\"accuracy :\",cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svmclf = svm.SVC(C = 1, kernel = 'rbf', gamma = 20, decision_function_shape = 'ovo', probability=True)\n",
    "cross_val_score(svmclf, X_train, y_train, scoring='roc_auc', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pred = svmclf.predict_proba(X_test)\n",
    "svmResult = pd.DataFrame({'Id': testIds,'Predicted': svm_pred[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmResult.to_csv('results/svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmclf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfclf = RandomForestClassifier()\n",
    "cross_val_score(rfclf, X_train, y_train, scoring='roc_auc', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rfclf.predict_proba(X_test)\n",
    "rfResult = pd.DataFrame({'Id': testIds,'Predicted': rf_pred[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfResult.to_csv('results/rf.csv', index=False) # all zeros and ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtclf = DecisionTreeClassifier()\n",
    "cross_val_score(dtclf, X_train, y_train, scoring='roc_auc', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = dtclf.predict_proba(X_test)\n",
    "dtResult = pd.DataFrame({'Id': testIds,'Predicted': dt_pred[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtResult.to_csv('results/dt.csv', index=False) # all zeros and ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbclf = GaussianNB()\n",
    "cross_val_score(nbclf, X_train, y_train, scoring='roc_auc', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pred = nbclf.predict_proba(X_test)\n",
    "nbResult = pd.DataFrame({'Id': testIds,'Predicted': nb_pred[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbResult.to_csv('results/nb.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "features = ['LP','Betweeness', 'Eigenvector', 'AdamicAdar']\n",
    "#features = ['LP','AdamicAdar']\n",
    "X_train1 = trainDF[features]\n",
    "y_train = trainDF['Label']\n",
    "X_test1 = testDF[features]\n",
    "testIds = testDF['Id']\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train1)\n",
    "X_test = sc.transform(X_test1)\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "print(cross_val_score(lr, X_train, y_train, scoring='roc_auc', cv=10))\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = list(trainDF['Label'])\n",
    "y_pred = list(predictions[:, 1])\n",
    "sns.distplot(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame(data={'Id': testIds, 'Predicted': predictions[:,1]})\n",
    "final_result.to_csv('results/LogisticRegression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = pd.DataFrame({\n",
    "    'features': importantFeatures,\n",
    "    'importance': lr.coef_[0]\n",
    "})\n",
    "l.plot.bar(x='features', rot=90).set_title('Feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_cv=GridSearchCV(\n",
    "    LogisticRegression(class_weight=\"balanced\"),\n",
    "    {\n",
    "        \"C\":np.logspace(-3,3,7),\n",
    "        \"penalty\":[\"l1\", \"l2\"],\n",
    "        \n",
    "    },\n",
    "    cv=10,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain=xgb.DMatrix(X_train,label=y_train)\n",
    "dtest=xgb.DMatrix(X_test)\n",
    "num_round=50\n",
    "parameters={'max_depth':7, 'eta':1, 'silent':1,'objective':'binary:logistic','eval_metric':'auc','learning_rate':.05}\n",
    "xg=xgb.train(parameters,dtrain,num_round)\n",
    "xgbPredictions=xg.predict(dtest)\n",
    "xgbResult = pd.DataFrame(data={'Id': testIds, 'Predicted': xgbPredictions})\n",
    "xgbResult.to_csv('results/XGBoost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "train_data=lgb.Dataset(X_train,label=y_train)\n",
    "num_round=50\n",
    "parameters = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'num_threads' : 2,\n",
    "    'learning_rate':0.05,        \n",
    "    'num_leaves': 40,        \n",
    "    'num_threads': 2,\n",
    "    'seed': 90051\n",
    "}\n",
    "lgbm=lgb.train(parameters,train_data,num_round)\n",
    "lgbmPredictions=lgbm.predict(X_test)\n",
    "lgbmResult = pd.DataFrame(data={'Id': testIds, 'Predicted': lgbmPredictions})\n",
    "lgbmResult.to_csv('results/lightGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('cv results = ', lgb.cv(\n",
    "        parameters,\n",
    "        train_data,\n",
    "        num_boost_round=10,\n",
    "        nfold=5,\n",
    "        metrics='auc',\n",
    "        early_stopping_rounds=10,\n",
    "        stratified=False\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic, svm, multi-layer perceptron\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "estimators = [\n",
    "    ('MLP', MLPClassifier(hidden_layer_sizes=(3), momentum=True, max_iter=300)),\n",
    "    ('SVM', svm.SVC(C = 1, kernel = 'rbf', gamma = 20, decision_function_shape = 'ovo', probability=True)),\n",
    "    ('LR', LogisticRegression(class_weight=\"balanced\"))\n",
    "]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "predictions = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame(data={'Id': testIds, 'Predicted': predictions[:,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv('results/StackedClassifiers11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "(\\_/)\n",
    "( •o•)\n",
    "/>  > \n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
